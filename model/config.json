{
    "pretrained_model_name_or_path":"GSAI-ML/LLaDA-8B-Instruct",
    "embed":{
        "LLaDA": "",
        "Qwen3": "embed_tokens"
    },
    "lm_head":{
        "LLaDA": "",
        "Qwen3": "lm_head"
    },
    "lora":{
        "enable": true,
        "start_layer": 26,
        "r": 8,
        "alpha": 16,
        "dropout": 0.05,
        "target_modules": {
            "LLaDA": ["q_proj", "k_proj", "v_proj","up_proj","attn_out"],
            "Qwen3": ["q_proj", "k_proj", "v_proj", "o_proj","gate_proj", "up_proj", "down_proj"]
        }
    },
    "length": 8,
    "talk_model": {
        "activation_type": "silu",
        "alibi": false,
        "alibi_bias_max": 8.0,
        "attention_dropout": 0.0,
        "attention_layer_norm": false,
        "attention_layer_norm_with_affine": true,
        "bias_for_layer_norm": false,
        "block_group_size": 1,
        "block_type": "llama",
        "d_model": 4096,
        "embedding_dropout": 0.0,
        "embedding_size": 126464,
        "eos_token_id": 126081,
        "flash_attention": false,
        "include_bias": false,
        "include_qkv_bias": false,
        "init_cutoff_factor": null,
        "init_device": "meta",
        "init_fn": "mitchell",
        "init_std": 0.02,
        "input_emb_norm": false,
        "layer_norm_type": "rms",
        "layer_norm_with_affine": true,
        "mask_token_id": 126336,
        "max_sequence_length": 4096,
        "mlp_hidden_size": 12288,
        "mlp_ratio": 4,
        "multi_query_attention": null,
        "n_heads": 32,
        "n_kv_heads": 32,
        "n_layers": 2,
        "pad_token_id": 126081,
        "precision": "amp_bf16",
        "residual_dropout": 0.0,
        "rms_norm_eps": 1e-05,
        "rope": true,
        "rope_full_precision": true,
        "rope_theta": 500000.0,
        "scale_logits": false,
        "vocab_size": 126464,
        "weight_tying": false
    },
    "use_residual": true,
    "state_dir": "lora-2-decoder-layer-r-s-d-8-32-2/state_3"
}